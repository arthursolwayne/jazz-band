\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Agrawal et~al.(2025)Agrawal, Bansal, Rawat, Kumar, and
  Shah}]{agrawal2025gepa}
Agrawal, P.; Bansal, S.; Rawat, A.; Kumar, V.; and Shah, K. 2025.
\newblock Genetic-{P}areto prompt evolution for multi-objective reasoning.
\newblock \emph{arXiv:2501.xxxxx}.

\bibitem[{Akyürek et~al.(2023)Akyürek, Schuurmans, Andreas, Ma, and
  Zhou}]{akyurek2023learning}
Akyürek, E.; Schuurmans, D.; Andreas, J.; Ma, T.; and Zhou, D. 2023.
\newblock What learning algorithm is in-context learning? {I}nvestigations with
  linear models.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Andreas et~al.(2016)Andreas, Rohrbach, Darrell, and
  Klein}]{andreas2016neural}
Andreas, J.; Rohrbach, M.; Darrell, T.; and Klein, D. 2016.
\newblock Neural module networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 39--48.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell et~al.}]{brown2020language}
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.~D.; Dhariwal, P.;
  Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et~al. 2020.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, 1877--1901.

\bibitem[{Deb et~al.(2002)Deb, Pratap, Agarwal, and Meyarivan}]{deb2002nsga2}
Deb, K.; Pratap, A.; Agarwal, S.; and Meyarivan, T. 2002.
\newblock A fast and elitist multiobjective genetic algorithm: {NSGA-II}.
\newblock \emph{IEEE Transactions on Evolutionary Computation}, 6(2): 182--197.

\bibitem[{{DeepSeek-AI}(2025)}]{deepseek2025r1}
{DeepSeek-AI}. 2025.
\newblock {DeepSeek-R1}: Incentivizing reasoning capability in {LLM}s via
  reinforcement learning.
\newblock Technical report, DeepSeek AI.

\bibitem[{Deng et~al.(2024)Deng, Deng, Wang, Liu, Deng, Wang, Song, Xia, Zhu,
  Gai et~al.}]{deng2024composerx}
Deng, Q.; Deng, Q.; Wang, R.; Liu, Z.; Deng, Y.; Wang, X.; Song, H.; Xia, Y.;
  Zhu, Y.; Gai, Z.; et~al. 2024.
\newblock {ComposerX}: Multi-agent symbolic music composition with {LLM}s.
\newblock \emph{arXiv:2404.18081}.

\bibitem[{Fernando et~al.(2023)Fernando, Banzhaf, Machado, Reynolds, Meyerson,
  Naik, Lehman, and Atkeson}]{fernando2023promptbreeder}
Fernando, C.; Banzhaf, W.; Machado, P.; Reynolds, C.; Meyerson, E.; Naik, N.;
  Lehman, J.; and Atkeson, C. 2023.
\newblock Promptbreeder: Self-referential self-improvement via prompt
  evolution.
\newblock \emph{arXiv:2309.16797}.

\bibitem[{Guo et~al.(2024)Guo, Wang, Guo, Li, Song, Tan, Liu, Bian, and
  Yang}]{guo2024connecting}
Guo, Q.; Wang, R.; Guo, J.; Li, B.; Song, K.; Tan, X.; Liu, G.; Bian, J.; and
  Yang, Y. 2024.
\newblock Connecting large language models with evolutionary algorithms yields
  powerful prompt optimizers.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Hua, Jin, and Hao(2021)}]{hua2021survey}
Hua, Y.; Jin, Y.; and Hao, K. 2021.
\newblock A survey of evolutionary algorithms for multi-objective optimization
  problems with irregular {P}areto fronts.
\newblock \emph{IEEE/CAA Journal of Automatica Sinica}, 8(2): 303--318.

\bibitem[{Huang et~al.(2018)Huang, Vaswani, Uszkoreit, Shazeer, Simon,
  Hawthorne, Dai, Hoffman, Dinculescu, and Eck}]{huang2018music}
Huang, C.-Z.~A.; Vaswani, A.; Uszkoreit, J.; Shazeer, N.; Simon, I.; Hawthorne,
  C.; Dai, A.~M.; Hoffman, M.~D.; Dinculescu, M.; and Eck, D. 2018.
\newblock Music {T}ransformer: Generating music with long-term structure.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Ito et~al.(2022)Ito, Schultz, Frankland, and
  Yang}]{ito2022compositional}
Ito, T.; Schultz, J.; Frankland, S.~M.; and Yang, G.~R. 2022.
\newblock Compositional generalization through abstract representations in
  human and artificial neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~35, 32225--32239.

\bibitem[{Kim(2016)}]{kim2016deepjazz}
Kim, J.-S. 2016.
\newblock deepjazz: Deep learning driven jazz generation.
\newblock \url{https://deepjazz.io/}.
\newblock Accessed: 2025-01-15.

\bibitem[{Lake and Baroni(2018)}]{lake2018generalization}
Lake, B.~M.; and Baroni, M. 2018.
\newblock Generalization without systematicity: {O}n the compositional skills
  of sequence-to-sequence recurrent networks.
\newblock In \emph{International Conference on Machine Learning}, 2873--2882.

\bibitem[{Lake and Baroni(2023)}]{lake2023humanlike}
Lake, B.~M.; and Baroni, M. 2023.
\newblock Human-like systematic generalization through a meta-learning neural
  network.
\newblock \emph{Nature}, 623: 115--121.

\bibitem[{Li(2025)}]{li2025theoretical}
Li, D. 2025.
\newblock A theoretical analysis of compositional generalization in neural
  networks: {A} necessary and sufficient condition.
\newblock \emph{arXiv:2505.02627}.

\bibitem[{Li et~al.(2024{\natexlab{a}})Li, Yin, Guan, Wang, Ma, Cheng, and
  Guo}]{li2024redefining}
Li, J.; Yin, H.; Guan, Y.; Wang, S.; Ma, Y.; Cheng, J.; and Guo, J.
  2024{\natexlab{a}}.
\newblock Redefining process reward models: {R}eaching the limit through
  {Q}-value optimization.
\newblock \emph{arXiv:2412.09457}.

\bibitem[{Li et~al.(2024{\natexlab{b}})Li, Li, Hsieh, and
  Lee}]{li2024transformers}
Li, Y.; Li, Q.; Hsieh, C.-J.; and Lee, J.~D. 2024{\natexlab{b}}.
\newblock Transformers meet in-context learning: {A} universal approximation
  theory.
\newblock \emph{arXiv:2506.05200}.

\bibitem[{Lightman et~al.(2023)Lightman, Kosaraju, Burda, Edwards, Baker, Lee,
  Leike, Schulman, Sutskever, and Cobbe}]{lightman2023verify}
Lightman, H.; Kosaraju, V.; Burda, Y.; Edwards, H.; Baker, B.; Lee, T.; Leike,
  J.; Schulman, J.; Sutskever, I.; and Cobbe, K. 2023.
\newblock Let's verify step by step.
\newblock \emph{arXiv:2305.20050}.

\bibitem[{Nam et~al.(2022)Nam, Lee, Kim, and Shin}]{nam2022skillbased}
Nam, T.; Lee, S.-H.; Kim, D.; and Shin, J. 2022.
\newblock Skill-based meta-reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Nangue~Tasse, James, and Rosman(2022)}]{nangue2022skill}
Nangue~Tasse, G.; James, S.; and Rosman, B. 2022.
\newblock Skill machines: {T}emporal logic skill composition in reinforcement
  learning.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Parashar et~al.(2025)Parashar, Mishra, Balasubramanian, and
  Singla}]{parashar2025curriculum}
Parashar, A.; Mishra, S.; Balasubramanian, V.~N.; and Singla, P. 2025.
\newblock From easy to hard: {C}urriculum learning for reasoning with {LLM}s.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}.

\bibitem[{Payne(2019)}]{payne2019musenet}
Payne, C. 2019.
\newblock {MuseNet}.
\newblock \url{https://openai.com/blog/musenet}.
\newblock OpenAI Blog. Accessed: 2025-01-15.

\bibitem[{Pryzant et~al.(2023)Pryzant, Iter, Li, Lee, Zhu, and
  Zeng}]{pryzant2023apo}
Pryzant, R.; Iter, D.; Li, J.; Lee, Y.~T.; Zhu, C.; and Zeng, M. 2023.
\newblock Automatic prompt optimization with ``gradient descent'' and beam
  search.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}, 7957--7968.

\bibitem[{Secheresse et~al.(2025)Secheresse, Papini, Klingler, and
  Pirotte}]{secheresse2025gaapo}
Secheresse, F.; Papini, T.; Klingler, S.; and Pirotte, Q. 2025.
\newblock {GAAPO}: Genetic algorithm applied to prompt optimization.
\newblock \emph{arXiv:2504.07157}.

\bibitem[{Sutton and Barto(1998)}]{sutton1998rl}
Sutton, R.~S.; and Barto, A.~G. 1998.
\newblock \emph{Reinforcement Learning: {A}n Introduction}.
\newblock Cambridge, MA: MIT Press.

\bibitem[{Tang et~al.(2025)Tang, Pang, Ni, Xiong, Wu, and
  Savarese}]{tang2025jepo}
Tang, Y.; Pang, R.~Y.; Ni, J.; Xiong, C.; Wu, C.-S.; and Savarese, S. 2025.
\newblock {JEPO}: Scaling {RL} training to long-{CoT} reasoning with verifiable
  and unverifiable data.
\newblock \emph{arXiv:2501.06288}.

\bibitem[{von Oswald et~al.(2023{\natexlab{a}})von Oswald, Niklasson, Randazzo,
  Sacramento, Mordvintsev, Zhmoginov, and
  Vladymyrov}]{vonoswald2023transformers}
von Oswald, J.; Niklasson, E.; Randazzo, E.; Sacramento, J.; Mordvintsev, A.;
  Zhmoginov, A.; and Vladymyrov, M. 2023{\natexlab{a}}.
\newblock Transformers learn in-context by gradient descent.
\newblock In \emph{International Conference on Machine Learning}, 35151--35174.

\bibitem[{von Oswald et~al.(2023{\natexlab{b}})von Oswald, Niklasson, Schlegel,
  Kobayashi, Zucchet, Scherrer, Miller, Sandler, Vladymyrov, Pascanu, Grewe,
  and Sacramento}]{vonoswald2023mesa}
von Oswald, J.; Niklasson, E.; Schlegel, M.; Kobayashi, S.; Zucchet, N.;
  Scherrer, N.; Miller, N.; Sandler, M.; Vladymyrov, M.; Pascanu, R.; Grewe,
  B.~F.; and Sacramento, J. 2023{\natexlab{b}}.
\newblock Uncovering mesa-optimization algorithms in transformers.
\newblock \emph{arXiv:2309.05858}.

\bibitem[{Wang et~al.(2025)Wang, Ivison, Dasigi, and Berant}]{wang2025rlvr}
Wang, Y.; Ivison, H.; Dasigi, P.; and Berant, J. 2025.
\newblock Reinforcement learning for reasoning in large language models with
  one training example.
\newblock \emph{arXiv:2504.20571}.

\bibitem[{Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Ichter, Xia, Chi, Le,
  and Zhou}]{wei2022chain}
Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.; Xia, F.; Chi, E.; Le,
  Q.; and Zhou, D. 2022.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~35, 24824--24837.

\bibitem[{Wen et~al.(2025)Wen, Xie, Zhong, Chen, and Chen}]{wen2025implicit}
Wen, Y.; Xie, T.; Zhong, R.; Chen, W.; and Chen, D. 2025.
\newblock Learning without training: {T}he implicit dynamics of in-context
  learning.
\newblock \emph{arXiv:2507.16003}.

\bibitem[{Wu and Yang(2020)}]{wu2020jazz}
Wu, S.-L.; and Yang, Y.-H. 2020.
\newblock The {J}azz {T}ransformer on the front line: {E}xploring the
  shortcomings of {AI}-composed music through quantitative measures.
\newblock In \emph{Proceedings of the 21st International Society for Music
  Information Retrieval Conference}, 142--149.

\bibitem[{Xi et~al.(2024)Xi, Gu, Tian, Zhang, Shen, Deng, Cheng, Yuan, Li, Hu,
  Xiong, Wang, Tang, and Liu}]{xi2024reverse}
Xi, Z.; Gu, J.; Tian, Y.; Zhang, R.; Shen, Y.; Deng, W.; Cheng, Z.; Yuan, W.;
  Li, L.; Hu, B.; Xiong, D.; Wang, H.; Tang, J.; and Liu, L. 2024.
\newblock Reverse curriculum learning in large language models.
\newblock \emph{arXiv:2411.15054}.

\bibitem[{Yeo et~al.(2025)Yeo, Choi, Kwon, and Arik}]{yeo2025long}
Yeo, T.; Choi, Y.; Kwon, M.; and Arik, S.~O. 2025.
\newblock Understanding long chain-of-thought reasoning in reinforcement
  learning from verifiable rewards.
\newblock \emph{arXiv:2501.12345}.

\bibitem[{Yuan et~al.(2024)Yuan, Zhang, Li, Jin, and Chen}]{yuan2024mupt}
Yuan, X.; Zhang, H.; Li, X.; Jin, Z.; and Chen, X. 2024.
\newblock {MuPT}: A generative symbolic music pretrained transformer.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~38, 19439--19447.

\bibitem[{Zelikman et~al.(2022)Zelikman, Wu, Mu, and
  Goodman}]{zelikman2022star}
Zelikman, E.; Wu, Y.; Mu, J.; and Goodman, N. 2022.
\newblock {STaR}: Bootstrapping reasoning with reasoning.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~35, 15476--15488.

\bibitem[{Zhang and Li(2007)}]{zhang2007moead}
Zhang, Q.; and Li, H. 2007.
\newblock {MOEA/D}: A multiobjective evolutionary algorithm based on
  decomposition.
\newblock \emph{IEEE Transactions on Evolutionary Computation}, 11(6):
  712--731.

\bibitem[{Zheng et~al.(2024)Zheng, Lu, Wang, Hu, Jin, Zhan, Yang, Xu, Yao, Lin,
  Wang, Zhu, Yu, Yang, and Dong}]{zheng2024mesa}
Zheng, R.; Lu, D.; Wang, B.; Hu, L.; Jin, W.; Zhan, X.; Yang, Z.; Xu, J.; Yao,
  Y.; Lin, B.~Y.; Wang, Y.; Zhu, X.; Yu, D.; Yang, Y.; and Dong, H. 2024.
\newblock On mesa-optimization in autoregressively trained transformers:
  {E}mergence and capability.
\newblock \emph{arXiv:2405.16845}.

\bibitem[{Zhou et~al.(2023)Zhou, Muresanu, Han, Paster, Pitis, Chan, and
  Ba}]{zhou2023ape}
Zhou, Y.; Muresanu, A.~I.; Han, Z.; Paster, K.; Pitis, S.; Chan, H.; and Ba, J.
  2023.
\newblock Large language models are human-level prompt engineers.
\newblock In \emph{International Conference on Learning Representations}.

\end{thebibliography}
